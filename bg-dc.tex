\section{Distributed Concurrency}
\label{bg-dc}

\subsection{Local Concurrency and Distributed Concurrency}

\input{fig-zk1264}

A well-known concurrency that developers most familiar with is from thread
execution interleaving in multi-threaded software or what we call in this
dissertation ``local concurrency''. Multi-threaded software has become common in
the age of multi-core processor, however, building multi-threaded software is
hard. Developers need to handle all possible interleaving of multiple threads
that are accessing to same data properly, otherwise concurrency bugs will
happen. These bugs are timing-related and non-deterministic, and they are
extremely difficult to test and debug.

For distributed systems, other than local concurrency, the systems are also
subject to ``distributed concurrency'' that is caused from interleaving of
computations in multiple nodes. Nodes in distributed systems do not have shared
memory and they access data in other nodes via network communication, so
basically, distributed concurrency comes from concurrent message arrivals and
internal computations in running nodes.

Other than timing of message arrivals and local computations, concurrent bugs in
distributed systems are caused from timing of failures as well. Cloud software
is often deploy on commodity hardware for vertical scaling purpose (Section
\ref{xxx}). This commodity hardware is unreliable, and hardware failures are not
an option \xxx{citation as Thanh did}. Cloud distributed systems need to
response to these failures, they need to detect and recover from the failures
and makes sure that users's data will not be lost or corrupted. Gauranteeing
this correctness is proven to be hard \xxx{cite FATE, Thanh's disser},
cloud-scale distributed systems need to handle failures that can happen at any
time and at any state of the systems. Some ordering of message arrivals could
make systems into state that developers never anticipate and is prone to error
when failure happens.

Consider hardware failures, concurrency bugs in cloud-scale distributed systems
is not only about interleaving of message arrivals and local computations, but
also timing of hardware failures as well. Figure \ref{fig-zk1264} shows an
example of a concurrency bug that happens because of untimely ordering of
message arrivals and node crashes. This bugs surfaces only if a follower
receives an UPTODATE message (step 14) after a commit message (step 12), and the
follower crashes before it does snapshot (step 20); only untimely message
arrivals or the timing of follower crash is not enough for bug to surface.

\subsection{Distributed Systems Model Checker (DMCK)}
\label{sec-bg-dmck}

In order to unearth DC bugs the question we have to answer is: ``{\em can we
exercise necessary conditions (\ie workloads and faults) and test different
event re-ordering to hit the bugs?}''. This is the job of distributed system
model checkers (dmck), which are gaining popularity recently
\cite{Guo+11-Demeter, Killian+07-LifeDeathMaceMC, Simsa+10-Dbug,
Yang+09-Modist}. Dmck works by intercepting distributed events and permuting
their ordering, and hereby pushing the target system into corner-case situations
and unearthing hard-to-find bugs. However, the more events included, the more
scalability issues will arise due to state-space explosion.

\input{fig-dmck}

The last ten years have seen a rise of software model checker that checks
distributed systems directly at the implementation level.  Figure~\ref{fig-dmck}
illustrates a dmck integration to a target distributed system, a simple
representation of existing dmck frameworks~\cite{Guo+11-Demeter,
Killian+07-LifeDeathMaceMC, Simsa+10-Dbug, Yang+09-Modist}.  The dmck inserts an
interposition layer in each node of the target system with the purpose of
controlling all important events (\eg, network messages, timeouts) and
preventing the target system to process the events until the dmck enables them.
A main dmck mechanism is the permutation of events; the goal is to push the
target system into all possible ordering scenarios.  For example, the dmck can
enforce \ts{abcd} ordering in one execution, \ts{bcad} in another, and so on.

\if 0 

\subsection{State-of-the-Art DMCKs}

\modist~\cite{Yang+09-Modist} is arguably one of the most powerful
dmcks that comes with systematic reduction policies.  \modist\ has been
integrated to real systems due to its exploration
scalability.  At the heart of \modist\ is {\em dynamic partial order
  reduction (DPOR)}~\cite{Flanagan+05-Dpor} which exploits the {\em
  independence} of events to reduce the state explosion.  Independent
events mean that it does not matter in what order the system execute
the events, as their different orderings are considered equivalent.

To illustrate how \modist\ adopts DPOR, let's use the example in
Figure~\ref{fig-dmck}, which shows four concurrent 
outstanding messages \ts{abcd}
(\ma\ and \mb\ for \none, \mc\ and \md\ for \ntwo).  A brute-force
approach will try all possible combinations (\ts{abcd}, \ts{abdc},
\ts{acbd}, \ts{acdb}, \ts{cabd}, and so on), for a total of 4!
executions.
% dpor
Fortunately, the notion of event independence can be mapped to
distributed system properties.  For example, \modist\ specifies this
reduction policy: a message to be processed by a given node is
independent of other concurrent messages destined to other nodes
(based on vector clocks).  Applying this policy to the example in
Figure~\ref{fig-dmck} implies that \ma\ and \mb\ are
dependent\footnote[1]{In model checking, ``dependent'' events mean
  that they must be re-ordered.  ``Dependent'' does not mean
  ``causally dependent''.}  but they are independent of \mc\ and
\md\ (and vice versa).  Since only dependent events need to be
reordered, this reduction policy leads to only 4 executions
(\ma\mb-\mc\md, \ma\mb-\md\mc, \mb\ma-\mc\md, \mb\ma-\md\mc), giving a
6x speed-up (4!/4).

Although \modist's speed-up is significant, we find that one 
scalability limitation of its DPOR application is within its {\em
  black-box} approach; it only exploits general properties of
distributed systems to define message independence.  It does not
exploit any semantic information from the target system to define more
independent events.  

% demeter
Dynamic interface reduction (DIR)~\cite{Guo+11-Demeter} is the next
advancement to \modist.  This work suggests that a complete dmck must
re-order not only messages (global events) but also thread
interleavings (local events).  The reduction intuition behind DIR is
that different thread interleavings often lead to the same global
events (\eg, a node sends the same messages regardless of how threads are
interleaved in that node).  DIR records local exploration and replays
future incoming messages without the need for global exploration.
In our work, SAMC focuses only on global exploration (message and fault
re-orderings).  We believe DIR is orthogonal to SAMC, similar to the
way DIR is orthogonal to \modist.

% besides (quick take)
\modist\ and DIR are examples of dmcks that employ advanced systematic
reduction policies.  LMC~\cite{Guerraoui+11-McNoNetwork} is similar to
DIR; it also decouples local and global exploration.
dBug~\cite{Simsa+10-Dbug} applies DPOR similarly to \modist.  There are
other dmcks such as \macemc~\cite{Killian+07-LifeDeathMaceMC} and
CrystalBall~\cite{Yabandeh+09-CrystalBall} that use basic exploration
methods such as depth first (DFS), weight-based,
and random searches.

% symmetry
Other than the aforementioned methods, {\em symmetry} is another
foundational reduction policy~\cite{Emerson+97-PorAndSym,
  Prasad+00-SymBasedMc}.  Symmetry-based methods exploit the
architectural symmetry present in the target system.  For example, in
a ring of nodes, one can rotate the ring without affecting
the behavior of the system.  Symmetry is powerful, but
we find no existing dmcks that adopt symmetry.

% multiple failures
Besides dmcks, there exists sophisticated testing frameworks for
distributed systems (\eg, \fate~\cite{Gunawi+11-FateDestini},
\prefail~\cite{Joshi+11-PreFail},
\setsudo~\cite{Joshi+13-SetsudoTesting}, OpenStack
fault-injector~\cite{Ju+13-FaultResOpenStack}). This set of work
emphasizes the importance of multiple faults, but their major
limitation is that they are not a dmck.  That is, they cannot
systematically control and permute non-deterministic choices such as
message and fault reorderings.

\fi
