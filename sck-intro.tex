%Talk about scale test.

In the previous chapter, we discuss an urgency and motivation to check
scalability of cloud distributed systems. From our observations in the previous
chapter plus what we discuss in Chapter \ref{chp-bg}, the effective way for
scalability checking is to check actual implement of the systems at real large
scale, not simulation nor small scale setup, and that makes emulation approach
as a desire choice. The limitation of emulation is resource contention which
does not allow developers to test \textit{really large} scale. Some works
address this limitaion, for example, Exalt \cite{exalt} addresses IO and storage
contention so developers can test I/O- and space-intensive protocols like
read/write protocols in HDFS.

While Exalt targets data paths and I/O emulation, 47\%\footnote{The other 53\%
are unexpected serializations of $O(N)$ operations.} of the scalability bugs
that we studied involve complex scale-dependent CPU computations in data and
control paths, which are not addressed in existing literature. Thus, in this
chapter, we introduce \sck\, a methodology that allows developers to colocate
severals nodes in a single machine and test CPU-intensive protocols

%In this section, we will explore state of the art of large-scale emulation.

We discuss the-state-of-the-art techniques to test scalability
and their limitations in Section \ref{mot-state}, and propose \sck, a
methodology to reveal scalability bugs in distributed systems economically by
using only a single machine in Section \ref{sec-sck}. We also evaluate how
effective and accurate \sck\ is compared to real large-scale testing in Section
\ref{sec-sck-eval}.

\section{State of the Art of Large-Scale Emulation}
\label{mot-state}

%As we explain in Chapter \ref{chp-bg} and show in Chapter \ref{chp-scb}, we need
%to check actual implement of the systems at real large scale, not simulation nor
%small scale setup, and that makes emulation approach as a desire choice. In this
%section, we will explore state of the art for large-scale emulation.

% --------------- emulation
%Real-scale emulation checks real implementations in an emulated
%environment.
%
DieCast \cite{Gupta+08-DieCast}, invented for network emulation, can colocate
many processes/VMs on a single machine as if they run individually without
contention.  The trick is adding ``time dilation factor'' (TDF) support
\cite{Gupta+06-TimeDilation} into the VMM (\eg, Xen).
%
For example, TDF=5 implies that for every second of wall-clock time, each
emulated VM on the VMM believes that time has advanced by only 200 ms.
%
The most significant drawback of DieCast is that high colocation factor (\eg,
TDF$=$100) is likely not desirable, for two reasons: prolonged testing time
(TDF$=$100 implies 100x longer run) and memory overcapacity.  DieCast was only
evaluated with TDF=10.


% co-location -- data compression -- exalt
Exalt \cite{Wang+14-Exalt} targets I/O-intensive scalability bugs.  With a
custom data compression, users' data is compressed to zero byte on disk (but the
size is recorded) while metadata is not compressed.  With this, Exalt can
colocate 100 emulated HDFS datanodes on one machine.  In its evaluation, most
of the bugs reproduced are in the HDFS namenode which runs alone on one machine.
As the authors stated, their approach ``may not discover scalability problems
that arise at the nodes that are being emulated [the datanodes]'' (\sec4.1 in
\cite{Wang+14-Exalt}). Thus, Exalt is not suitable for finding scalability bugs
in CPU-intensive distributed systems. 


% P2P systems \cite{sosp01-past}.

In summary, we did not find a fast and accurate single-machine approach that can
scale-check CPU-intensive protocols in cloud systems.
%
The scalability bugs could be caused by the scale-dependent processing time, not
network or I/O bottlenecks. As DieCast targets {\em network} emulation via time
dilation and Exalt targets {\em storage} space emulation via compression. 

%\sck uniquely targets {\em processing time} emulation, completing a missing
%piece.

