\section{Bug Study}

Bug or failure studies can significantly guide many aspects of dependability
research. Many dependability researchers have recently employed formal studies
on bugs and failures \cite{Guo+13-CureIsWorse, Li+13-ScopeBugStudy}. These
studies can identify opportunities for new research, build taxonomies of new
problems, or test new tools. We start our work by doing formal bug study to gain
foundations of bugs in cloud systems.

\subsection{Cloud Bug Study (Initial Work)}

As an initiative, our group have performed the largest bug study in six
important Apache cloud infrastructures including Cassandra, Flume, Hadoop
MapReduce, HBase, HDFS, and ZooKeeper \cite{Gunawi+14-Cbs}. We reviewed in total
21,399 submitted issues within a three-year period (2011-2014).  We perform a
deep analysis of 3,655 ``vital'' issues (\ie, real issues affecting deployments)
with a set of detailed classifications.  \subsubsection{Methodology}

\input{tab-tag}

\begin{itemize}

\item {\bf Target Systems \& Bug Repositories:} We select six popular cloud
systems that represent a diverse set of system architectures: Hadoop
MapReduce~\cite{HadoopWeb} (distributed computing frameworks), Hadoop File
System (HDFS)~\cite{HDFSWeb} (scalable storage systems), HBase~\cite{HBaseWeb}
and Cassandra~\cite{CassandraWeb} (distributed NoSQL systems),
ZooKeeper~\cite{ZooKeeperWeb} (synchronization services), and finally
Flume~\cite{FlumeWeb} (streaming systems).
%
All development projects of the target systems maintain highly organized issue
repositories\footnote{Hadoop MapReduce in particular has two repositories
(Hadoop and MapReduce). The first one contains mostly development
infrastructure (\eg, UI, library) while the second one contains system issues.
We use the latter.}.
%
Each repository contains development and deployment issues submitted mostly by
the developers or a larger user community. The term ``issue'' is used here to
represent both bugs and new features.

\if 0
For every issue, the repository stores many ``raw'' labels. There are five
options for {\em bug priority} label: trivial, minor, major, critical, and
blocker. We label the first two as ``minor'' and the last three as ``major''.
Although we analyze all issues in our work, we only focus on major issues in
our analysis.
\fi

\item {\bf Issue Classifications:} We introduce issue classifications as
displayed in Table~\ref{tab-tag}. 
%
We carefully read each issue to decide whether the issue is vital. If an issue
is vital we proceed with further classifications, otherwise it is labeled as
miscellaneous and skipped in our study. 

\end{itemize}

\subsubsection{Study Result}

The finding of this study was published in \cbs paper \cite{Gunawi+14-Cbs}.  The
product of our classifications is stored in \cdb, a set of raw text files, data
mining scripts and graph utilities \cite{CBS}, which enables us (and other \cdb\
users) to perform both quantitative and qualitative analysis of cloud issues. 

This study brings new insights on some of the most vexing problems in cloud
systems. We show a wide range of intricate bugs, many of which are unique to
distributed cloud systems (\eg, scalability, topology, and killer bugs). And it
is the main source of our DC-bug taxonomy and scalability-bug analysis.

\subsection{DC Bug Taxonomy} 

While there have been many LC-bug studies, I am not aware of any large-scale
study of DC bugs. A recent study from Microsoft analyzed the effect of
distributed concurrency on workload and only studied five DC bugs in MapReduce
systems \cite{Xiao+14-NonDetMR}. To fill the void, I as one of the project
leaders, have created the largest and most comprehensive taxonomy of 104
real-world DC bugs (named \taxdc) from Cassandra, HBase, Hadoop MapReduce/Yarn,
and ZooKeeper \cite{Leesatapornwongsa+16-TaxDC}. \taxdc\ contains in-depth
characteristics of DC bugs, stored in the form of 2,083 classification labels
and 4,528 lines of re-enumerated steps to the bugs that I manually added.
Motivated by the availability of bug benchmarks for LC bugs, I will release
\taxdc\ as a large-scale DC bugs benchmark.

With \taxdc\, I can answer important questions such as: How often are DC bugs
reported from real deployments? What types of DC bugs exist in real world?
What are the root causes of DC bugs (out-of-order messages, failures, \etc)?
Are existing LC-bug-detection tools applicable for DC bugs? How do developers
fix DC bugs (by adding locks, states, \etc)? What are the inputs/triggering
conditions?  What are the minimum number of distributed events needed to
trigger the bugs (how many messages to re-order, failures to inject, \etc)?
What errors/effects (specification violations) are caused by DC bugs (deadlock,
data loss, state inconsistency, performance problems, \etc)? How do propagation
chains form from the root causes to errors? The answers to these questions will
guide my subsequent research projects.

