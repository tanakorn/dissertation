\section{Scalability}
\label{bg-sc}

\subsection{Vertical Scaling vs Horizontal Scaling} 
\label{bg-sc-type}

When systems' workload grows (the number of users raises or individual users'
requests increase), developers need to scale the systems to add more capability
and keep users satisfied. Two traditional approaches to scale system are used as
we show below \cite{Michael+07-ScaleUpXScaleOut}:
\begin{itemize}

\item \textbf{Vertical scaling or scale-up}: this approach expands system
capabilities by adding more resources (\eg, CPU, memory, and storage) to a
single node to boost its performance, and make software to leverage additional
resources. For example, run more processes of applications in the node.

\item \textbf{Horizontal scaling or scale-out}: this approach enhance the
capability by adding more nodes to current distributed systems to yield higher
aggregate capability; mostly, the nodes that we are adding are low-cost
machines.

\end{itemize}

In the past, vertical scaling was widely favored by many companies.
Multiprocessor with higher clock rate can satisfy computing power need of
largest companies \cite{Michael+07-ScaleUpXScaleOut}. Vertical scaling requires
less human effort than horizontal scaling; it does not need more administrative
effort because the number of machines and systems administrators need to handle
is still the same. The disadvantages of vertical scaling are the upgradability
is limited by existing hardware manufacturing, and the upgrade cost is
expensive.

Because of the upgradability and price issues, nowadays, the trend goes to
horizontal scaling. Many cloud service companies adopt this approach (\eg,
Google, Facebook, Amazon, \etc). The cost of horizontal scaling is much more
cheaper than the vertical scaling and there is not limitation for hardware to
scale out infinitely (the limitations are posed by software stack)
\cite{ScaleUpVsScaleOut}. In addition, hardware manufacturers try to facilitate
scale-out approach \cite{Michael+07-ScaleUpXScaleOut}.

\subsection{Scalability Testing}

As we discussed in \ref{bg-sc-type}, horizontal scaling or scale-out is a trend
now because it does not expose hardware limitation. The limitation is in
software stack so developers need to invent scalable algorithms and protocols,
however, before real deployment, if they do not have a large cluster to test
their implementations, there could be ``\textit{scalability bugs}'' hide there.

We now discuss popular approaches (simulation, extrapolation, and emulation) for
unearthing scalability bugs that avoid acquiring a number of machines, because
testing on such deployments is costly.
% ......
First, simulation approaches test system/application models in different scales
\cite{Calotoiu+13-ApmScaleBug, Laguna+15-DebugAtScale}, However, a model can
look scalable but the actual implementation can contain unforeseen bugs. Later
in Section \ref{mot-bug}, we will show our observations from \textit{real-world}
scalability bugs that simulation cannot detect them.

Second, extrapolation monitors system behaviors in ``mini clusters'' and
extrapolates them to larger scales (\sec2.1 in \cite{Wang+14-Exalt}). However,
mini clusters tend to be order(s) of magnitude smaller than real deployments.
Most importantly, system behaviors do not always extrapolate linearly
\cite{Wang+14-Exalt}. 

Finally, real-scale emulation checks real implementations in an emulated
environment \cite{Gupta+08-DieCast, Wang+14-Exalt}. This approach emulates real
large-scale system in a single machine. For example, a naive way to achieve this
is just colocating multiple processes/VMs on one machine. The limitation here is
emulation consumes real resources (CPU, memory, and storage), so with limited
resources, we can not emulate \textit{really large} deployment (\eg, can test up
to 50-node deployment). Some works try to resolve this resource contention issue
\cite{Gupta+08-DieCast, Wang+14-Exalt}. We will discuss them in Section
\ref{mot-state}

\subsection{Scalability Benchmarking}

Scalability benchmarking (\eg, YCSB~\cite{Cooper+10-YCSB}) is a standard method
to check throughput/latency scalability.  The results are useful for
advertising system capabilities, and thus acquiring a large number of machines
is justifiable. But this is different from testing and debugging code to check
system scalability.

%however is a different matter; for every
%scalability bug, developers need to debug the code in tens/hundreds of
%iterations.  A single-machine debugging approach is valuable.
