\section{Scalability}
\label{bg-sc}

\subsection{Vertical Scaling vsa Horizontal Scaling} 

When systems' workload grows (the number of users raises or individual users'
requests increase), developers need to scale the systems to add more capability
and keep users satisfied. Two traditional approaches to scale system are used as
we show below \cite{Michael+07-ScaleUpXScaleOut}:
\begin{itemize}

\item \textbf{Vertical scaling or scale-up}: this approach expands system
capabilities by adding more resources (\eg, CPU, memory, and storage) to a
sing node to boost its performance, and make software to leverage additional
resources. For example, run more processes of applications in the node.

\item \textbf{Horizontal scaling or scale-out}: this approach enhance the
capability by adding more nodes to current distributed systems to yeild higher
aggregate capability; mostly, the nodes that we are adding are low-cost
machines.

\end{itemize}

In the past, vertical scaling was widely favored by many companies.
Multiprocessor with higher clock rate can satisfy computing power need of
largest companies \cite{Michael+07-ScaleUpXScaleOut}. Vertical scaling requires
less human effort than horizontal scaling; it does not need more administrative
effort because the number of machines and systems administrators need to handle
is still the same. The disadvantages of vertical scaling are the upgradability
is limited by existing hardware manufacturing, and the upgrade cost is
expensive.

Because of the upgradability and price issues, nowadays, the trend goes to
horizontal scaling. Many cloud service companies adopt this approache (\eg,
Google, Facebook, Amazon, \etc). The cost of horizontal scaling is much more
cheaper than the vertical scaling and there is not limitation for hardware to
scale out infinitely (the limitations are posed by software stack)
\cite{ScaleUpVsScaleOut}. In addition, hardware manufaturers try to facilitate
scale-out approach \cite{Michael+07-ScaleUpXScaleOut}.

\subsection{Scalability Testing}

We now discuss popular approaches (simulation, extrapolation, and emulation) for
unearthing scalability bugs.
% ......
First, simulation approaches test system/application models in different scales
\cite{Calotoiu+13-ApmScaleBug, Laguna+15-DebugAtScale}. A model can look
scalable but the actual implementation can contain unforeseen bugs. Our
observations above accentuate the need for scale-checking distributed system
{\em implementations} at {\em real scale}.

Second, extrapolation monitors system behaviors in ``mini clusters'' and
extrapolates them to larger scales (\sec2.1 in \cite{Wang+14-Exalt}).  However,
mini clusters tend to be order(s) of magnitude smaller than real deployments.
Most importantly, system behaviors do not always extrapolate linearly
\cite{Wang+14-Exalt}. 

Finally, real-scale emulation checks real implementations in an emulated
environment (\eg, DieCast and Exalt).  DieCast \cite{Gupta+08-DieCast},
invented for network emulation, can colocate many processes/VMs on a single
machine as if they run individually without contention.  The trick is adding
``time dilation factor'' (TDF) support \cite{Gupta+06-TimeDilation} into the
VMM (\eg, Xen).  For example, TDF=5 implies that for every second of wall-clock
time, each emulated VM on the VMM believes that time has advanced by only 200
ms.  The most significant drawback of DieCast is that high colocation factor
(\eg, TDF$=$100) is likely not desirable, for two reasons: prolonged testing
time (TDF$=$100 implies 100x longer run) and memory overcapacity.  Many
distributed systems today are implemented in managed languages (\eg, Java,
Erlang) whose runtimes consume non-negligible memory overhead. Java and Erlang
VMs for example use around 70 and 64 MB of memory per process respectively.
DieCast was only evaluated with TDF=10.

Exalt \cite{Wang+14-Exalt} targets I/O-intensive scalability bugs.  With a
custom data compression, users' data is compressed to zero byte on disk (but the
size is recorded) while metadata is not compressed.  With this, Exalt can
co-locate 100 emulated HDFS datanodes on one machine.  In its evaluation, most
of the bugs reproduced are in the HDFS namenode which runs alone on one machine.
As the authors stated, their approach ``may not discover scalability problems
that arise at the nodes that are being emulated [the datanodes]'' (\sec4.1 in
\cite{Wang+14-Exalt}).  Thus, Exalt is not suitable for finding control-plane
scalability bugs in P2P distributed systems.

In summary, we did not find a fast single-machine approach that can scale-check
control-plane protocols in P2P systems.  The scalability bugs here are
typically caused by the scale-dependent processing time, not network or I/O
bottlenecks.  As DieCast targets {\em network} emulation via time dilation and
Exalt targets {\em storage} space emulation via compression, our work uniquely
targets {\em processing time} emulation, completing a missing piece.

