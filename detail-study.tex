\section{Bug Study}

Bug or failure studies can significantly guide many aspects of dependability
research. Many dependability researchers have recently employed formal studies
on bugs and failures \cite{Guo+13-CureIsWorse, Li+13-ScopeBugStudy}. These
studies can identify opportunities for new research, build taxonomies of new
problems, or test new tools. We start our work by doing formal bug study to gain
foundations of bugs in cloud systems.

\subsection{Cloud Bug Study (Previous Work)}

As an initiative, our group have performed the largest bug study in six
important Apache cloud infrastructures including Cassandra, Flume, Hadoop
MapReduce, HBase, HDFS, and ZooKeeper \cite{Gunawi+14-Cbs}. We reviewed in total
21,399 submitted issues within a three-year period (2011-2014).  We perform a
deep analysis of 3,655 ``vital'' issues (\ie, real issues affecting deployments)
with a set of detailed classifications.  \subsubsection{Methodology}

\input{tab-tag}

\begin{itemize}

\item {\bf Target Systems \& Bug Repositories:} We select six popular cloud
systems that represent a diverse set of system architectures: Hadoop
MapReduce~\cite{HadoopWeb} (distributed computing frameworks), Hadoop File
System (HDFS)~\cite{HDFSArchitecture} (scalable storage systems),
HBase~\cite{HBaseWeb} and Cassandra~\cite{CassandraWeb} (distributed NoSQL
systems), ZooKeeper~\cite{ZooKeeperWeb} (synchronization services), and finally
Flume~\cite{FlumeWeb} (streaming systems).
%
All development projects of the target systems maintain highly organized issue
repositories.
%
Each repository contains development and deployment issues submitted mostly by
the developers or a larger user community. The term ``issue'' is used here to
represent both bugs and new features.

\if 0
For every issue, the repository stores many ``raw'' labels. There are five
options for {\em bug priority} label: trivial, minor, major, critical, and
blocker. We label the first two as ``minor'' and the last three as ``major''.
Although we analyze all issues in our work, we only focus on major issues in
our analysis.
\fi

\item {\bf Issue Classifications:} We introduce issue classifications as
displayed in Table~\ref{tab-tag}. 
%
We carefully read each issue to decide whether the issue is vital. If an issue
is vital we proceed with further classifications, otherwise it is labeled as
miscellaneous and skipped in our study. 

\end{itemize}

\subsubsection{Study Result}

The finding of this study was published in the \cbs\ paper
\cite{Gunawi+14-Cbs}.  The product of our classifications is stored in \cdb\
\cite{CBSWeb}, a set of raw text files, data mining scripts and graph utilities,
which enables us (and other \cdb\ users) to perform both quantitative and
qualitative analysis of cloud issues. 

This study brings new insights on some of the most vexing problems in cloud
systems. We show a wide range of intricate bugs, many of which are unique to
distributed cloud systems (\eg, scalability, topology, and killer bugs). And it
is the main source of our DC-bug taxonomy and scalability-bug analysis.

\subsection{DC Bug Taxonomy}

While there have been many LC-bug studies, we are not aware of any large-scale
study of DC bugs. To fill the void, we have created the largest and most
comprehensive taxonomy of 104 real-world DC bugs (named \taxdc) from four
distributed system: Cassandra, HBase, Hadoop MapReduce/Yarn, and ZooKeeper
\cite{Leesatapornwongsa+16-TaxDC}.

\subsubsection{Methodology}

\input{tab-tax}

\begin{itemize}

\item {\bf Target Systems \& Bug Repositories:} We examined bugs from four
distributed systems that represent a diverse set of system architectures: Hadoop
MapReduce \cite{HadoopWeb} (distributed computing frameworks), HBase
\cite{HBaseWeb} and Cassandra \cite{CassandraWeb} (distributed NoSQL systems),
and ZooKeeper \cite{ZooKeeperWeb} (synchronization services).
%
We started our study from \cdb\ \cite{CBSWeb}, which already labels issues
related to concurrency bugs. However, beyond simple labeling, the \cbs\ work does
not differentiate DC from LC bugs and did not dissect DC bugs further.
Thus, we first filtered out LC bugs, then exclude DC bugs that do not contain
clear description, and finally randomly picked \numDcBugs\ samples from the
remaining detailed DC bugs, specifically \numDcCA\ Cassandra, \numDcHB\ HBase,
\numDcMR\ Hadoop MapReduce, and \numDcZK\ ZooKeeper DC bugs, reported in January
2011-2014 (the time range of CBS work).  

\item {\bf Taxonomy:} We study the characteristics of DC bugs along three key
stages: triggering, errors \& failures, and fixing as shown in Table \ref{tab:tax}.

\end{itemize}

\subsubsection{Study Result}

We publish the result in \taxdc\ paper \cite{Leesatapornwongsa+16-TaxDC}, and we
also release \taxdc\ database online \cite{CBSWeb}.  \taxdc\ contains in-depth
characteristics of DC bugs, stored in the form of 2,083 classification labels
and 4,528 lines of re-enumerated steps to the bugs that we manually added. 

With \taxdc, we can answer important questions such as: What are the root causes
of DC bugs (out-of-order messages, failures, \etc)?  Are existing
LC-bug-detection tools applicable for DC bugs? How do developers fix DC bugs (by
adding locks, states, \etc)? What are the inputs/triggering conditions?  What
are the minimum number of distributed events needed to trigger the bugs (how
many messages to re-order, failures to inject, \etc)?  What errors/effects
(specification violations) are caused by DC bugs (deadlock, data loss, state
inconsistency, performance problems, \etc)? How do propagation chains form from
the root causes to errors? The answers to these questions guide our subsequent
research projects.

\subsection{Scalability Bug Study}
\label{sc-study}

For scalability bugs, the situation is even worse. We are not aware of any study
on scalability bugs at all. We started a pilot study of scalability bugs to gain
some insight about them. We studied 12 bugs in four key-value stores including
Cassandra, Couchbase, Riak, and Voldemort.

\subsubsection{Methodology}

\begin{itemize}

\item {\bf Target Systems \& Bug Repositories} Focusing on control-plane
protocol in P2P systems, we chose four popular P2P key-value stores to study:
Cassandra, Couchbase, Riak, and Voldemort. Because \cbs\ is a study aiming on
different kinds of systems, the only P2P key-value store \cbs\ addresses is
Cassandra. We have to mine bug repositories for the others manually.  And we
found \numStudy\ control-plane scalability bugs.  (This manual mining was arduous
because there is no standard jargon for ``scalability bugs''; we might have
missed other related bugs.)

\item {\bf Detail Study} In each bug, we studied the problematic protocol in
several aspects: protocol design, implementation, problem root cause, symptom,
and fix.

\end{itemize}

\subsubsection{Preliminary Result}

From the study, we can gain some insight about scalability bugs in control-plane
protocols. We show the observations in Section \ref{sec-scobs}. These
observations guide us how to create a methodology for checking scalability of
the systems.

