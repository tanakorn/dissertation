\section{Scale Emulation}

Next we move on to scalability bugs. Our observations in Section \ref{sec-scobs}
accentuate the need for scale checking distributed system implementations at
real scale, not via simulation nor extrapolation. 
%
Our proposed solution is to colocate as many nodes as possible (\eg, hundreds)
on one machine without sacrificing accuracy by {\em emulate} hardware resources
such that the individual nodes behave as if they run on independent machines.
We are introducing \sck\, a scale-check methodology for control-plane protocols
in P2P key-value systems.

We first summarize our target problem, challenges, and principles in tackling
the problem.

\begin{itemize}

% ------------------------------------------
\item {\bf Target problem/protocols:} We focus on scale-checking P2P
control-plane protocols (\eg, bootstrapping, rebalancing, scale out/down). The
scalability bugs here tend to be caused by CPU-intensive computation on data
structures that are dependent on cluster scale.

% ------------------------------------------------------
\item {\bf The challenge:} A major challenge here is
%
``how can we {\em colocate hundreds of CPU-intensive and memory-hungry nodes} on
one machine with limited resources and yet still achieve {\em high accuracy} and
{\em unprolonged debugging time}?''
%
High accuracy implies that the colocated nodes observe a similar behavior as if
they run on independent machines.
%
Unprolonged debugging time implies that if a scalability bug surfaces in $T$
minutes in real deployment, it must also be observable within a similar time
frame during the reproduction of the bug (unlike time dilation impacts in
DieCast).

\item {\bf Principles:}
%
Each distributed system has its own unique protocols, and implementation
details. Thus, we introduce \sck\ as a {\em general methodology} that can be
applied to various P2P systems in unique ways.
%
We also show how distributed systems should be re-architected to make them {\em
scale-checkable} (``\sck-able'') on one machine.
%
\sck\ is not designed to depend on custom operating systems or libraries (\eg,
time-dilated VMM or compression library).

\end{itemize}


\subsection{Proposed Emulation Techniques}

\begin{itemize}

\item {\bf Processing Illusion (PIL):}
%
To emulate CPU-intensive processing, we introduce {\em processing
illusion} (PIL), an approach that {\em replaces an actual processing
with \sleep}.  For example, in \caone, we can replace the expensive
ring-table update with \ts{sleep(t)} where \ts{t} is an accurate timing of
how long the update takes.

The intuition behind PIL is similar to the intuition behind other emulation
techniques.
%
For example, Exalt provides an illusion of storage space; their insight was
``how data is processed is  not affected by the content of the data being
written, but only by its size'' \cite{Wang+14-Exalt}.
%
PIL provides an illusion of compute processing; our insight is that {\em ``the
key to computation is not the intermediate results, but rather the execution
time and eventual output''.}

To make PIL feasible, these are challenging questions that we are going to
answer.

\begin{enumerate}

\item How can a function be safely replaced with \sleep \textit{without}
changing the whole processing semantic?
 
\item How to find specific functions that should be replaced with \sleep?
 
\item How can we produce the output if the actual compute is skipped?
 
\item How can we predict the actual compute time (\ts{t}) accurately?

\end{enumerate}


\item {\bf Single-Process Cluster (SPC):} The next challenge that we address is
an overhead due to running multiple processes. For example, Voldemort
\cite{VoldemortWeb} are implemented in Java, which each JVM consumes
non-negligible memory overhead (70 MB). As we target 3-digit colocation factor,
this memory overhead becomes an unnecessary limitation.
%
Furthermore, a managed-language VM can contain advanced services.  For example,
Erlang VM contains a DNS service which sends heartbeat messages to other
connected VMs.  As hundreds of Erlang VMs (one for each Riak node) run on one
machine, the heartbeat messages cause a ``network'' overflow that disconnects
Erlang VMs.

To address this, we propose Single-Process Cluster (SPC) support wherein the
whole cluster runs as threads in a single process. But SPC is not naive change.
When we look at our target systems, this cannot be done without re-designing the
code to support SPC. We introduce \sck-able architecture, in which systems can
run in SPC mode. For example, systems should have arrays of per-node global data
structures, and be rid of static-synchronized functions that lock the whole
cluster when run in SPC mode.

Moreover, if we can make all nodes run in one process, user-kernel switching to
send messages becomes unnecessary. Thus, we can create a shim layer in our
target systems to bypass OS network calls to reduce this overhead.


\item {\bf Global Event Driven Architecture (GEDA):} Next, we can further SPC by
reducing thread switching overhead. With SPC, a node still runs multiple daemon
threads (gossiper, failure detector, \etc). With high colocation factor, there
are more than one thousand threads that cause severe context switching and long
queuing delays.

We address this overhead by leveraging the staged event-driven architecture
(SEDA) \cite{Welsh+01-Seda} common in distributed system implementations.  With
SEDA, each service/stage in each node exclusively has an event queue and a
handler thread. In \sck mode, we convert SEDA to {\em global-event driven
architecture} (GEDA). That is, for every stage, there is only {\em one} queue
and one handler for the {\em whole} cluster.

This architecture will reduce the number of threads in \sck run, and mitigate
thread context switching overhead.

\item {\bf Memory Footprint Reduction (MFR):} The last thing we can perform is
memory footprint reduction (MFR). We propose that we can reduce memory overhead
from system-specific root causes to prevent out-of-memory exceptions.

First, relevant services in the target protocol can ``over-allocate'' memory.
%
For example, in Riak's bootstrap+rebalance protocol, each node creates
$N$$\times$$P$ partition services although at the end only retain $P$ partitions
and never use (remove) the other $(N$$-$$1)$$\times$$P$ partitions (as reclaimed
by other nodes).
%
Worse, each partition service is an Erlang process (1.3 MB of memory overhead);
colocating 30 nodes ($N$$=$30 with default $P$$=$64) will directly consume 75 GB
of memory (30$\times$30$\times$64$\times$1.3 MB) from the start.
%
To reduce this memory usage, we must modify Riak to remove this unoptimized
memory usage.

Second, some libraries can cause high memory footprints. For example, Voldemort
nodes use Java NIO \cite{VoldemortNIO} which is fast but contains buffers and
connection metadata that take up memory space. We should change this to
network bypass from SPC.



\end{itemize}

We are working on these four proposed techniques. PIL is the most challenging
technique to implement, and it is the most important technique to scale check
CPU-intensive protocols. There are many questions we need to answer to make PIL
possible.
%
And as we mentioned, some techniques need to re-design the codebase to make the
techniques possible, but we believe the modification is not complex and will be
small.

