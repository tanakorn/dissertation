


\subsection{Newer Versions and New Bugs}
\label{eval-new}



We also scale-checked the latest stable versions of Cassandra (v2.2.5),
HDFS (v2.7.3), Riak (v2.1.3), and Voldemort (v1.10.21), and found new bugs
in Cassandra and HDFS, one in each.


For Cassandra, \sfind points us to nested scale-dependent loops again,
within the code paths of bootstrapping, scaling-out, and decommissioning
protocols.
%
We create the corresponding test cases and \sck\ shows that cluster-wide
flapping resurfaces again but only observable in 512-node deployment.
%
As an example, decommissioning just only one node already caused almost
100,000 flaps.  We submitted the bug to the developers and they 
acknowledge that it is another design problem (more below).
%
To prevent flappings, the developers suggested us to add/remove node one
at a time with 2-minute separation, which means scaling-out/down 100 nodes
will take over 3 hours.  Put it in another way, scalability bugs make
instant elasticity not achievable.
%
Cassandra developers recently started a new initiative and opened a new
``umbrella'' ticket for designing ``Gossip 2.0'' \cite{Gossip20}, supposed
to scale to
% https://issues.apache.org/jira/browse/CASSANDRA-12345
1000+ nodes \cite{Gossip20Mail}.
% http://mail-archives.apache.org/mod_mbox/cassandra-dev/201609.mbox/%3CCAHjqPuJMkfZwp9DDX45PNBNhkoGXsPW4TFT6Zxv%2BTTz_Pg3Y%2Bg%40mail.gmail.com%3E
% maybe create a short google url ?
%
% The bug affects bootstrap, scale-out, and decommission protocols 
%
%We just submitted this new bug to the developers.  We can reproduce it in
%\sck and are currently debugging the root cause together with the
%developers.  


For Riak and Voldemort, we found that their latest-stable
bootstrap/rebalance protocols do not exhibit any scalability bug, up to
512 nodes.




% -------------------------------- HDFS

% Huan's data:
% goo.gl/ElTTRO

For HDFS, first, \sfind does not find any instance of the special
worker-to-master ``loop'' (\sec\ref{sc-find}) that is within the client
read/write call paths as in bug \hdone (\sec\ref{eval-bugs}f).
%
% if have space
% For example, when \sfind analyzed the old version, it pointed us to the
% synchronized \ts{DataNodeProtocol.blockReceivedAndDeleted} in the master
% node, which is called within the frequently-called write paths.
%
%When we apply the same analysis to HDFS v2.7.3, \sfind does not find such
%special loops.
%
%
However, \sfind found another instance of a scale-dependent loop within a
locking function (confirmed by the developers).  Specifically, \sfind
reports the following number of lines executed:

\vminfive
{\footnotesize
\begin{verbatim}
    DatanodeManager.refreshDatanodes  N*(136*B+137)   
    HeartbeatManager.heartbeatCheck   N*36
    DatanodeManager.datanodeDump      N*16
    ...
\end{verbatim}
}


% DatanodeManager.datanodeDump      N*16
% DatanodeManager.getDnListForRep   N*16
% DatanodeManager.markAllDnStale    N*11
%DatanodeManager.clearPendQueues   N*8

Here, \sfind sorts the functions based on the number of lines executed.
``\ts{B}'' represents the number of blocks per datanode (\eg, 10,000).
%
Thus, as $N$ increases, \ts{refreshDatanodes} prolongs user requests in
acquiring the master lock.
%
From the code-path report (\sec\ref{sc-find}.3), the function is triggered
during {\em re}-commissioning.  This is reminiscent to the old
decommissioning bug \hdtwo (\sec\ref{eval-bugs}g); in this older version,
\sfind reports a similar finding: \ts{FSNamesys.decommissionDatanodeCheck}
in \ts{N*(159*B+33)} lines.
%
In summary, this section proves that \sck is also effective in finding new
bugs.



\if 0

Loops in locks:


...

Old Version v-0.17.2
FSNamesystem.refreshNodes  176B+81 4.4*10^9
FSNamesystem.decommissionedDatanodeCheck159B+334.0*10^9
FSNamesystem.datanodeDump 3115500


...

  3.4*10^9
\fi
